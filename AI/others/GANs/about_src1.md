src: https://towardsdatascience.com/generative-models-and-gans-fe7efc20020b

# Intro.
- Simply they are a class of Unsupervised Machine Learning Models which are used to generate some Data.
- It uses the Joint Probability Distribution over the observations.
- GANs have the potential and power to automatically learn the features from a dataset , categories or dimentions or anything and generate data.
- Right now they are hot because of their highly useful and Practical Applications in real time and the power they have to almost learn anything.

## Joint Probability.
- Joint probability is the probability of two events happening together.
- p(A and B) or p(A ∩ B).
- Joint probability is also called the intersection of two (or more) events. The intersection can be represented by a Venn diagram.
- **Example**: The probability that a card is a five and black = p(five and black) = 2/52 = 1/26. (There are two black fives in a deck of 52 cards, the five of spades and the five of clubs).
## Joint Distribution.
- A joint probability distribution shows a probability distribution for two (or more) random variables.
- f(x,y) = P(X=x, Y=y)
- The whole point of the joint distribution is to look for a relationship between two variables.
## Joint Probabililty Mass Function.
- If your variables are discrete (like in the above table example), their distribution can be described by a joint probability mass function (Joint PMF).
- Basically, if you have found all probabilities for all possible combinations of X and Y, then you have created a joint PMF.
## Joint Probability Density Function
- If you have continuous variables, they can be described with a probability density function (PDF).
- Unlike the discrete variable example above, you can’t write out every combination of every variable because you would have infinite possibilities to write out (which is, of course, impossible). What you can do is create a formula; **The formula that describes all possible combinations of X and Y** is called a joint PDF.

# Discriminative Models.
- Discriminative Models are conditional Models used for modelling the dependence of dependent variable(Y) on the dependent variables(X) using the Conditional Probabilities - Pr(Y | X) = Pr(X and Y)/Pr(X)= Pr( X | Y ).Pr(Y)/Pr(X) between both of them.

# More about GANs.
- GANs use the above 2 class of machine learning Models:
    1. Generative Models.
    2. Discriminative Models.
- Now we use Neural Nets as a Generative Model in GAN.
Using Generative Models on Neural Nets requres smaller number of parameters than the amount of data usually a Neural Net requires to be trained on. Hence the moodel does a good job at efficiently generating data.

# Traning a Generative Model.
- To train a Generative Model, we first collect lots of data of any domain (images, audio, videos etc) and then feed the data to the model to generate data like it.
- Example: we used a new-initialized network to generate 500 images, each time starting with a different random code. The question is that: how should we adjust the network’s parameters to encourage it to produce slightly more believable(real looking) samples in the future?
One clever approach to use here is using GAN(Generative Adversarial Network) , i.e introducing another Discriminator Model which discriminates or classifies the images that we feed it to Real or Fake images.
For a instance lets consider we can feed the Discriminator the Real image dataset and another set of Images generated from the Generator and use it as a standard classifier to discriminate or differentiate amongst the 2 types of images Real and Fake(generated images), and here’s the magic— we can also backpropagate through both the discriminator and the generator to find how we should change the generator’s parameters to make its 500 image samples slightly more confusing for the discriminator. And what actually happens by backpropogation is that the Generator and the Discriminator both are indulged in a battle of defeating each other in what they both are good at i.e The generator is trying to generate images and fool and confuse the Discriminator into believing that they are Real , and the discriminator tries distinguish the Real images from the Fake ones generated by the Generator
- In the end, the generator network is outputting images that are indistinguishable from real images for the discriminator.
- As we can see the learning process of a Generator — initially the Generator generates noisy and low resolution images , but due to backpropogation- over time they actually learn to generate better and better images which look completely real and indistinguishable.

# Summarize.
- GAN - Generative Adversarial Networks
- The training process is a fight between two Models: a Generator Model and a second discriminator Model
- Discriminator Model tries to classify samples as either coming from the true distribution p(x) or the model distribution P(X) using conditional probabilities.
- Every time the discriminator notices a difference between the two distributions the generator adjusts its parameters slightly to make it go away and make the images much better and real looking , until at the end the generator exactly reproduces the true data distribution and the discriminator is guessing at random, unable to find a difference.
