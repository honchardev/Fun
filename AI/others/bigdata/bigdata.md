# Hadoop core components:
- Основніми являются:
    - HDFS - Hadoop Distributed File System - распределённая файловая система, позволяющая хранить информацию практически неограниченного объёма.
    - Hadoop YARN - фреймворк для управления ресурсами кластера и менеджмента задач, в том числе включает фреймворк MapReduce.
    - Hadoop common.
- Также существует большое количество проектов непосредственно связанных с Hadoop, но не входящих в Hadoop core:
    - Hive – инструмент для SQL-like запросов над большими данными (превращает SQL-запросы в серию MapReduce–задач);
    - Pig – язык программирования для анализа данных на высоком уровне. Одна строчка кода на этом языке может превратиться в последовательность MapReduce-задач;
    - Hbase – колоночная база данных, реализующая парадигму BigTable;
    - Cassandra – высокопроизводительная распределенная key-value база данных; 
    - Apache Spark - движок для распределённой обработки данных; использует компоненты Hadoop, такие как HDFS и YARN для своей работы.

- HDFS - storage layer of Hadoop; a filesystem designed for storing very large files rather than small files.
- YARN - YARN refers as yet another resource negotiator; resource management layer of Hadoop; Yarn allows different data processing engines like graph processing, interactive processing, stream processing as well as batch processing to run and process data stored in HDFS. Yarn extends the power of Hadoop to other evolving technologies, so they can take the advantages of HDFS.

# MapReduce
- MapReduce is a programming model designed for processing large volumes of data in parallel by dividing the work into a set of independent tasks.
- Work (complete job) which is submitted by the user to master is divided into small works (tasks) and assigned to slaves.
- Here in map reduce we get input as a list and it converts it into output which is again a list

- 2 phases:
    1. Map phase (splits & mapping)
    2. Reduce phase (shuffling & reducing)

- Process
    1. Get an input
    2. Input Splits
        - input is divided into fixed-size pieces called input splits.
    3. Mapping
        - input: chunk of data from step 2 - a single split.
        - in this phase, data in each split is passed to a mapping function to produce output values.
        - example: count number of occurences of each word from input splits.
            - output in this case: pair <word, frequency>
    4. Shuffling
        - input: a single map
        - Main task: consolidate the relevant records from Mapping phase output.
        - example: frequency for each word
            - same words are clubed together along with their respective frequency
    5. Reducer
        - in this phase, output values from shuffling phase are AGGREGATED
        - Main task: combine values from shuffling phase and return a single output value
            - == summarize the complete dataset.
        - example: calculate total occurences of each words
    6. Final output

- Mapper example:
def mapper(input):
    for word in input.split():
        if word and len(word) > 1: #can use a stopwords list to do filter
            print (word, 1)
- Mapper X output:
('MapReduce', 1)
('is', 1)
('widely', 1)
('used', 1)
('to', 1)
...

- Reducer example:
def reducer(input):
    word_freq = {}
    pre = None
    cur_freq = 0 # record the frequency for current word
    for word, freq in input:
        # a new word
        if not pre or word == pre:
            cur_freq += freq
        else:
            word_freq[pre] = cur_freq
            cur_freq = 1 # a new word
        pre = word
    word_freq[word] = cur_freq
    for w, freq in word_freq.iteritems():
        print '{0},{1}'.format(w, freq)
- Reducer output:
data,3
MapReduce,2
Hadoop,2

- Sorting with mapreduce
    - MapReduce makes the guarantee that the input to every reducer is sorted by key.
    - The process by which the system performs the sort—and transfers the map outputs to the reducers as inputs—is known as the shuffle.
    - In many ways, the shuffle is the heart of MapReduce and is where the magic happens.

- Shuffling in MapReduce
    - The process of transferring data from the mappers to reducers is shuffling
    - It is also the process by which the system performs the sort

- Sorting in MapReduce
    - MapReduce Framework automatically sort the keys generated by the mapper
    - Thus, before starting of reducer, all intermediate key-value pairs get sorted by key and not by value
    
- Shuffle step:
    - The shuffle step is actually the most important step.
    - That is where the MapReduce system takes over and you get all the performance if it is well implemented.

    - Mappers and reducers run in parallel, and independent of each other.
    - They usually do really stupid operations, such as in the word-count example

    - The shuffle phase is where all the heavy lifting occurs.
    - All the data is rearranged for the next step to run in parallel again.

    - you can extend with custom map and reduce functions, but not with a custom shuffle - that part needs to be written by experts, you can only modify the keys used by it.

    - It's also where people fail in writing good mapreduce. For example by producing too many copies before shuffling, or putting everything into the same key (shuffle does not make a lot of sense then anymore).

- Shuffle step:
    - The shuffle step occurs to guarantee that the results from mapper which have the same key (of course, they may or may not be from the same mapper) will be send to the same reducer. So, the reducer can further reduce the result set.

# Mapreduce usage examples
- basic: counting and summing
class Mapper
   method Map(docid id, doc d)
      for all term t in doc d do
         Emit(term t, count 1)
class Reducer
   method Reduce(term t, counts [c1, c2,...])
      sum = 0
      for all count c in [c1, c2,...] do
          sum = sum + c
      Emit(term t, count sum)

- Group By / Aggregations.
    - Here the advantage of the shuffling stage is clear.
    - An explanation that shuffling is also distributed sort + an explanation of distributed sort algorithm also helps.

- Join of two tables.
    - People working with DB are familiar with the concept and its scalability problem. Show how it can be done in MR.

- One set of familiar operations that you can do in MapReduce is the set of normal SQL operations: SELECT, SELECT WHERE, GROUP BY, ect.

- Another good example is matrix multiply, where you pass one row of M and the entire vector x and compute one element of M * x.

- other examples: https://highlyscalable.wordpress.com/2012/02/01/mapreduce-patterns/
